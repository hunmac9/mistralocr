# Docker Compose for GPU-enabled deployment using vLLM
# Use with: docker compose -f docker-compose.gpu.yml up -d --build
#
# Requirements:
# - NVIDIA GPU with CUDA support (6GB+ VRAM recommended)
# - NVIDIA Container Toolkit installed
#   - Linux: https://docs.nvidia.com/datacenter/cloud-native/container-toolkit/install-guide.html
#   - Windows (Docker Desktop): Enable WSL2 backend, install NVIDIA drivers in Windows
#     https://docs.docker.com/desktop/gpu/
#
# This configuration:
# - Uses vLLM for PaddleOCR-VL (official recommended deployment)
# - Exposes OpenAI-compatible API at port 8000
# - GPU required for vLLM

services:
  # Main application service
  app:
    build: .
    ports:
      - "${FLASK_PORT:-5009}:${FLASK_PORT:-5009}"
    environment:
      MISTRAL_API_KEY: ${MISTRAL_API_KEY:-}
      FLASK_PORT: ${FLASK_PORT:-5009}
      MAX_UPLOAD_MB: ${MAX_UPLOAD_MB:-100}
      MISTRAL_MAX_MB: ${MISTRAL_MAX_MB:-50}
      SERVER_NAME: ${SERVER_NAME:-localhost:${FLASK_PORT:-5009}}
      FLASK_DEBUG: ${FLASK_DEBUG:-False}
      OCR_BACKEND: ${OCR_BACKEND:-auto}
      LOCAL_OCR_URL: http://local-ocr:8000
      LOCAL_OCR_IDLE_TIMEOUT: ${LOCAL_OCR_IDLE_TIMEOUT:-300}
      LOCAL_OCR_AUTO_START: "false"
      # Use vLLM mode for GPU
      LOCAL_OCR_USE_VLLM: "true"
    depends_on:
      local-ocr:
        condition: service_healthy
    networks:
      - ocr-network
    command: ["gunicorn", "--bind", "0.0.0.0:${FLASK_PORT:-5009}", "--timeout", "600", "app:app"]

  # PaddleOCR-VL via vLLM (official deployment method)
  local-ocr:
    image: vllm/vllm-openai:v0.6.6.post1
    environment:
      HUGGING_FACE_HUB_TOKEN: ${HUGGING_FACE_HUB_TOKEN:-}
      # Disable multiprocessing which can fail on Windows/WSL2
      VLLM_WORKER_MULTIPROC_METHOD: spawn
    volumes:
      - vllm-cache:/root/.cache
    networks:
      - ocr-network
    # Required for GPU memory access in container
    ipc: host
    # GPU passthrough configuration
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/health"]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 600s
    # vLLM server command - optimized for Windows/WSL2
    command: >
      --model PaddlePaddle/PaddleOCR-VL
      --trust-remote-code
      --dtype bfloat16
      --enforce-eager
      --max-model-len 4096
      --gpu-memory-utilization 0.80
      --host 0.0.0.0
      --port 8000

networks:
  ocr-network:
    driver: bridge

volumes:
  vllm-cache:
