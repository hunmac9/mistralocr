# --- OCR Backend Configuration ---

# OCR Backend Selection (Default: auto)
# Options:
#   - "auto": Use local OCR by default, fall back to Mistral if API key is available
#   - "local": Use local OCR only (Surya for CPU, PaddleOCR-VL/Chandra for GPU)
#   - "mistral": Use Mistral OCR API only (requires API key)
#OCR_BACKEND=auto

# --- Mistral OCR Settings (Optional - only needed for Mistral backend) ---

# Your Mistral AI API Key
# Get one from https://console.mistral.ai/api-keys
# Not required if using local OCR backend
#MISTRAL_API_KEY=your_actual_api_key_here

# --- Local OCR Settings ---

# URL of the local OCR server (Default: http://localhost:8000)
# When using Docker Compose, this is set automatically to the container name
#LOCAL_OCR_URL=http://localhost:8000

# Local OCR model selection (Default: surya for CPU, paddle for GPU)
# Options:
#   - "surya": Surya OCR (~300M params) - CPU-friendly, recommended for CPU-only deployment
#   - "paddle": PaddleOCR-VL-0.9B - General purpose OCR (GPU recommended, transformers API has issues)
#   - "chandra": Chandra OCR (9B params) - Layout-preserving OCR (GPU required, 18GB+ VRAM)
# Note: Models are mutually exclusive - only one is loaded at a time
# Switching models will unload the current model first
#LOCAL_OCR_MODEL=surya

# Idle timeout in seconds before the model is unloaded from memory/VRAM (Default: 300)
# Set to 0 to keep the model loaded permanently
# When using GPU, the model will be unloaded from VRAM after this timeout
#LOCAL_OCR_IDLE_TIMEOUT=300

# Auto-start the local OCR container when needed (Default: true)
# Set to "false" when using Docker Compose (container managed by compose)
#LOCAL_OCR_AUTO_START=true

# Docker image for local OCR service (Default: mistralocr-paddleocr:latest)
#LOCAL_OCR_DOCKER_IMAGE=mistralocr-paddleocr:latest

# Container name for local OCR service (Default: mistralocr-paddleocr)
#LOCAL_OCR_CONTAINER_NAME=mistralocr-paddleocr

# --- GPU Configuration (Optional) ---
# To use GPU acceleration, use docker-compose.gpu.yml instead of docker-compose.yml:
#   docker compose -f docker-compose.gpu.yml up -d --build
#
# Requirements:
#   - NVIDIA GPU with CUDA support (6GB+ VRAM recommended)
#   - NVIDIA Container Toolkit installed
#   - Linux: Follow https://docs.nvidia.com/datacenter/cloud-native/container-toolkit/install-guide.html
#   - Windows: Docker Desktop with WSL2 backend + NVIDIA drivers
#
# GPU mode will automatically:
#   - Use CUDA for inference (10-50x faster than CPU)
#   - Unload model from VRAM after idle timeout
#   - Fall back to CPU if GPU is not available

# --- Server Settings ---

# Port the application will run on (Default: 5009)
#FLASK_PORT=5009

# Hostname and port for generating absolute URLs (e.g., download links)
# Should match how the app is accessed externally (e.g., localhost:5009, ocr.yourdomain.com)
# Defaults to localhost:{FLASK_PORT} if not set.
#SERVER_NAME=localhost:5009

# --- Upload Settings ---

# Maximum total upload size allowed by the Flask app in Megabytes (Default: 100)
#MAX_UPLOAD_MB=100

# Maximum file size accepted by the OCR backend in Megabytes (Default: 50)
# PDFs larger than this will be split into parts before processing.
#MISTRAL_MAX_MB=50

# --- API Rate Limiting ---

# Default rate limit for all API endpoints (Default: 100 per hour)
# Format: "{count} per {period}" where period is second/minute/hour/day
#RATE_LIMIT_DEFAULT=100 per hour

# Rate limit for OCR submission endpoint (Default: 10 per minute)
# This is more restrictive since OCR is resource-intensive
#RATE_LIMIT_OCR=10 per minute

# Rate limit storage backend (Default: memory://)
# Options:
#   - "memory://": In-memory storage (resets on restart, good for single instance)
#   - "redis://localhost:6379": Redis storage (persistent, good for multiple instances)
#   - "memcached://localhost:11211": Memcached storage
#RATE_LIMIT_STORAGE=memory://

# --- Development Settings (Optional) ---

# Enable Flask debug mode (Default: False). Set to 'True' to enable.
#FLASK_DEBUG=False
